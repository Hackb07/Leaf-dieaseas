{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "687db7fb-9a66-4e1b-a2cf-b1013a54d643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modules loaded\n"
     ]
    }
   ],
   "source": [
    "# import system libs\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import pathlib\n",
    "import itertools\n",
    "\n",
    "# import data handling tools\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# import Deep learning Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print ('modules loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b0f4c0a-b152-4a40-adc7-3e4332360754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data paths with labels\n",
    "def define_paths(data_dir):\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "\n",
    "    folds = os.listdir(data_dir)\n",
    "    for fold in folds:\n",
    "        foldpath = os.path.join(data_dir, fold)\n",
    "        # check the folders from main directory. If there are another files, ignore them\n",
    "        if pathlib.Path(foldpath).suffix != '':\n",
    "            continue\n",
    "\n",
    "        filelist = os.listdir(foldpath)\n",
    "        for file in filelist:\n",
    "            fpath = os.path.join(foldpath, file)\n",
    "\n",
    "            # check if there are another folders\n",
    "            if pathlib.Path(foldpath).suffix == '':\n",
    "                # check unneeded masks\n",
    "                if pathlib.Path(fpath).parts[-1] == 'masks' or pathlib.Path(fpath).parts[-1] == 'Masks' or pathlib.Path(fpath).parts[-1] == 'MASKS':\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    o_file = os.listdir(fpath)\n",
    "                    for f in o_file:\n",
    "                        ipath = os.path.join(fpath, f)\n",
    "                        filepaths.append(ipath)\n",
    "                        labels.append(fold)\n",
    "\n",
    "            else:            \n",
    "                filepaths.append(fpath)\n",
    "                labels.append(fold)\n",
    "\n",
    "    return filepaths, labels\n",
    "\n",
    "\n",
    "# Concatenate data paths with labels into one dataframe ( to later be fitted into the model )\n",
    "def define_df(files, classes):\n",
    "    Fseries = pd.Series(files, name= 'filepaths')\n",
    "    Lseries = pd.Series(classes, name='labels')\n",
    "    return pd.concat([Fseries, Lseries], axis= 1)\n",
    "\n",
    "# Split dataframe to train, valid, and test\n",
    "def split_data(data_dir):\n",
    "    # train dataframe\n",
    "    files, classes = define_paths(data_dir)\n",
    "    df = define_df(files, classes)\n",
    "    strat = df['labels']\n",
    "    train_df, dummy_df = train_test_split(df,  train_size= 0.8, shuffle= True, random_state= 123, stratify= strat)\n",
    "\n",
    "    # valid and test dataframe\n",
    "    strat = dummy_df['labels']\n",
    "    valid_df, test_df = train_test_split(dummy_df,  train_size= 0.5, shuffle= True, random_state= 123, stratify= strat)\n",
    "\n",
    "    return train_df, valid_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9953427e-e70c-4845-bf4e-180acd188e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gens (train_df, valid_df, test_df, batch_size):\n",
    "    '''\n",
    "    This function takes train, validation, and test dataframe and fit them into image data generator, because model takes data from image data generator.\n",
    "    Image data generator converts images into tensors. '''\n",
    "\n",
    "\n",
    "    # define model parameters\n",
    "    img_size = (224, 224)\n",
    "    channels = 3 # either BGR or Grayscale\n",
    "    color = 'rgb'\n",
    "    img_shape = (img_size[0], img_size[1], channels)\n",
    "\n",
    "    # Recommended : use custom function for test data batch size, else we can use normal batch size.\n",
    "    ts_length = len(test_df)\n",
    "    test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\n",
    "    test_steps = ts_length // test_batch_size\n",
    "\n",
    "    # This function which will be used in image data generator for data augmentation, it just take the image and return it again.\n",
    "    def scalar(img):\n",
    "        return img\n",
    "\n",
    "    tr_gen = ImageDataGenerator(preprocessing_function= scalar, horizontal_flip= True)\n",
    "    ts_gen = ImageDataGenerator(preprocessing_function= scalar)\n",
    "\n",
    "    train_gen = tr_gen.flow_from_dataframe( train_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
    "                                        color_mode= color, shuffle= True, batch_size= batch_size)\n",
    "\n",
    "    valid_gen = ts_gen.flow_from_dataframe( valid_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
    "                                        color_mode= color, shuffle= True, batch_size= batch_size)\n",
    "\n",
    "    # Note: we will use custom test_batch_size, and make shuffle= false\n",
    "    test_gen = ts_gen.flow_from_dataframe( test_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
    "                                        color_mode= color, shuffle= False, batch_size= test_batch_size)\n",
    "\n",
    "    return train_gen, valid_gen, test_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85240175-9902-42ff-9859-2cbd52f2e168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(gen):\n",
    "    '''\n",
    "    This function take the data generator and show sample of the images\n",
    "    '''\n",
    "\n",
    "    # return classes , images to be displayed\n",
    "    g_dict = gen.class_indices        # defines dictionary {'class': index}\n",
    "    classes = list(g_dict.keys())     # defines list of dictionary's kays (classes), classes names : string\n",
    "    images, labels = next(gen)        # get a batch size samples from the generator\n",
    "\n",
    "    # calculate number of displayed samples\n",
    "    length = len(labels)        # length of batch size\n",
    "    sample = min(length, 25)    # check if sample less than 25 images\n",
    "\n",
    "    plt.figure(figsize= (20, 20))\n",
    "\n",
    "    for i in range(sample):\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        image = images[i] / 255       # scales data to range (0 - 255)\n",
    "        plt.imshow(image)\n",
    "        index = np.argmax(labels[i])  # get image index\n",
    "        class_name = classes[index]   # get class of image\n",
    "        plt.title(class_name, color= 'blue', fontsize= 12)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7e437bf-619a-4cee-bece-6f0fc75068eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, model, patience, stop_patience, threshold, factor, batches, epochs, ask_epoch):\n",
    "        super(MyCallback, self).__init__()\n",
    "        self.model = model\n",
    "        self.patience = patience # specifies how many epochs without improvement before learning rate is adjusted\n",
    "        self.stop_patience = stop_patience # specifies how many times to adjust lr without improvement to stop training\n",
    "        self.threshold = threshold # specifies training accuracy threshold when lr will be adjusted based on validation loss\n",
    "        self.factor = factor # factor by which to reduce the learning rate\n",
    "        self.batches = batches # number of training batch to run per epoch\n",
    "        self.epochs = epochs\n",
    "        self.ask_epoch = ask_epoch\n",
    "        self.ask_epoch_initial = ask_epoch # save this value to restore if restarting training\n",
    "\n",
    "        # callback variables\n",
    "        self.count = 0 # how many times lr has been reduced without improvement\n",
    "        self.stop_count = 0\n",
    "        self.best_epoch = 1   # epoch with the lowest loss\n",
    "        self.initial_lr = float(tf.keras.backend.get_value(model.optimizer.lr)) # get the initial learning rate and save it\n",
    "        self.highest_tracc = 0.0 # set highest training accuracy to 0 initially\n",
    "        self.lowest_vloss = np.inf # set lowest validation loss to infinity initially\n",
    "        self.best_weights = self.model.get_weights() # set best weights to model's initial weights\n",
    "        self.initial_weights = self.model.get_weights()   # save initial weights if they have to get restored\n",
    "\n",
    "    # Define a function that will run when train begins\n",
    "    def on_train_begin(self, logs= None):\n",
    "        msg = 'Do you want model asks you to halt the training [y/n] ?'\n",
    "        print(msg)\n",
    "        ans = input('')\n",
    "        if ans in ['Y', 'y']:\n",
    "            self.ask_permission = 1\n",
    "        elif ans in ['N', 'n']:\n",
    "            self.ask_permission = 0\n",
    "\n",
    "        msg = '{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy', 'V_loss', 'V_acc', 'LR', 'Next LR', 'Monitor','% Improv', 'Duration')\n",
    "        print(msg)\n",
    "        self.start_time = time.time()\n",
    "\n",
    "\n",
    "    def on_train_end(self, logs= None):\n",
    "        stop_time = time.time()\n",
    "        tr_duration = stop_time - self.start_time\n",
    "        hours = tr_duration // 3600\n",
    "        minutes = (tr_duration - (hours * 3600)) // 60\n",
    "        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n",
    "\n",
    "        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n",
    "        print(msg)\n",
    "\n",
    "        # set the weights of the model to the best weights\n",
    "        self.model.set_weights(self.best_weights)\n",
    "\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs= None):\n",
    "        # get batch accuracy and loss\n",
    "        acc = logs.get('accuracy') * 100\n",
    "        loss = logs.get('loss')\n",
    "\n",
    "        # prints over on the same line to show running batch count\n",
    "        msg = '{0:20s}processing batch {1:} of {2:5s}-   accuracy=  {3:5.3f}   -   loss: {4:8.5f}'.format(' ', str(batch), str(self.batches), acc, loss)\n",
    "        print(msg, '\\r', end= '')\n",
    "\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs= None):\n",
    "        self.ep_start = time.time()\n",
    "\n",
    "\n",
    "    # Define method runs on the end of each epoch\n",
    "    def on_epoch_end(self, epoch, logs= None):\n",
    "        ep_end = time.time()\n",
    "        duration = ep_end - self.ep_start\n",
    "\n",
    "        lr = float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n",
    "        current_lr = lr\n",
    "        acc = logs.get('accuracy')  # get training accuracy\n",
    "        v_acc = logs.get('val_accuracy')  # get validation accuracy\n",
    "        loss = logs.get('loss')  # get training loss for this epoch\n",
    "        v_loss = logs.get('val_loss')  # get the validation loss for this epoch\n",
    "\n",
    "        if acc < self.threshold: # if training accuracy is below threshold adjust lr based on training accuracy\n",
    "            monitor = 'accuracy'\n",
    "            if epoch == 0:\n",
    "                pimprov = 0.0\n",
    "            else:\n",
    "                pimprov = (acc - self.highest_tracc ) * 100 / self.highest_tracc # define improvement of model progres\n",
    "\n",
    "            if acc > self.highest_tracc: # training accuracy improved in the epoch\n",
    "                self.highest_tracc = acc # set new highest training accuracy\n",
    "                self.best_weights = self.model.get_weights() # training accuracy improved so save the weights\n",
    "                self.count = 0 # set count to 0 since training accuracy improved\n",
    "                self.stop_count = 0 # set stop counter to 0\n",
    "                if v_loss < self.lowest_vloss:\n",
    "                    self.lowest_vloss = v_loss\n",
    "                self.best_epoch = epoch + 1  # set the value of best epoch for this epoch\n",
    "\n",
    "            else:\n",
    "                # training accuracy did not improve check if this has happened for patience number of epochs\n",
    "                # if so adjust learning rate\n",
    "                if self.count >= self.patience - 1: # lr should be adjusted\n",
    "                    lr = lr * self.factor # adjust the learning by factor\n",
    "                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) # set the learning rate in the optimizer\n",
    "                    self.count = 0 # reset the count to 0\n",
    "                    self.stop_count = self.stop_count + 1 # count the number of consecutive lr adjustments\n",
    "                    self.count = 0 # reset counter\n",
    "                    if v_loss < self.lowest_vloss:\n",
    "                        self.lowest_vloss = v_loss\n",
    "                else:\n",
    "                    self.count = self.count + 1 # increment patience counter\n",
    "\n",
    "        else: # training accuracy is above threshold so adjust learning rate based on validation loss\n",
    "            monitor = 'val_loss'\n",
    "            if epoch == 0:\n",
    "                pimprov = 0.0\n",
    "\n",
    "            else:\n",
    "                pimprov = (self.lowest_vloss - v_loss ) * 100 / self.lowest_vloss\n",
    "\n",
    "            if v_loss < self.lowest_vloss: # check if the validation loss improved\n",
    "                self.lowest_vloss = v_loss # replace lowest validation loss with new validation loss\n",
    "                self.best_weights = self.model.get_weights() # validation loss improved so save the weights\n",
    "                self.count = 0 # reset count since validation loss improved\n",
    "                self.stop_count = 0\n",
    "                self.best_epoch = epoch + 1 # set the value of the best epoch to this epoch\n",
    "\n",
    "            else: # validation loss did not improve\n",
    "                if self.count >= self.patience - 1: # need to adjust lr\n",
    "                    lr = lr * self.factor # adjust the learning rate\n",
    "                    self.stop_count = self.stop_count + 1 # increment stop counter because lr was adjusted\n",
    "                    self.count = 0 # reset counter\n",
    "                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) # set the learning rate in the optimizer\n",
    "\n",
    "                else:\n",
    "                    self.count = self.count + 1 # increment the patience counter\n",
    "\n",
    "                if acc > self.highest_tracc:\n",
    "                    self.highest_tracc = acc\n",
    "\n",
    "        msg = f'{str(epoch + 1):^3s}/{str(self.epochs):4s} {loss:^9.3f}{acc * 100:^9.3f}{v_loss:^9.5f}{v_acc * 100:^9.3f}{current_lr:^9.5f}{lr:^9.5f}{monitor:^11s}{pimprov:^10.2f}{duration:^8.2f}'\n",
    "        print(msg)\n",
    "\n",
    "        if self.stop_count > self.stop_patience - 1: # check if learning rate has been adjusted stop_count times with no improvement\n",
    "            msg = f' training has been halted at epoch {epoch + 1} after {self.stop_patience} adjustments of learning rate with no improvement'\n",
    "            print(msg)\n",
    "            self.model.stop_training = True # stop training\n",
    "\n",
    "        else:\n",
    "            if self.ask_epoch != None and self.ask_permission != 0:\n",
    "                if epoch + 1 >= self.ask_epoch:\n",
    "                    msg = 'enter H to halt training or an integer for number of epochs to run then ask again'\n",
    "                    print(msg)\n",
    "\n",
    "                    ans = input('')\n",
    "                    if ans == 'H' or ans == 'h':\n",
    "                        msg = f'training has been halted at epoch {epoch + 1} due to user input'\n",
    "                        print(msg)\n",
    "                        self.model.stop_training = True # stop training\n",
    "\n",
    "                    else:\n",
    "                        try:\n",
    "                            ans = int(ans)\n",
    "                            self.ask_epoch += ans\n",
    "                            msg = f' training will continue until epoch {str(self.ask_epoch)}'\n",
    "                            print(msg)\n",
    "                            msg = '{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy', 'V_loss', 'V_acc', 'LR', 'Next LR', 'Monitor', '% Improv', 'Duration')\n",
    "                            print(msg)\n",
    "\n",
    "                        except Exception:\n",
    "                            print('Invalid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74bdb5d9-40da-4e73-981d-582bdd383ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(hist):\n",
    "    '''\n",
    "    This function take training model and plot history of accuracy and losses with the best epoch in both of them.\n",
    "    '''\n",
    "\n",
    "    # Define needed variables\n",
    "    tr_acc = hist.history['accuracy']\n",
    "    tr_loss = hist.history['loss']\n",
    "    val_acc = hist.history['val_accuracy']\n",
    "    val_loss = hist.history['val_loss']\n",
    "    index_loss = np.argmin(val_loss)\n",
    "    val_lowest = val_loss[index_loss]\n",
    "    index_acc = np.argmax(val_acc)\n",
    "    acc_highest = val_acc[index_acc]\n",
    "    Epochs = [i+1 for i in range(len(tr_acc))]\n",
    "    loss_label = f'best epoch= {str(index_loss + 1)}'\n",
    "    acc_label = f'best epoch= {str(index_acc + 1)}'\n",
    "\n",
    "    # Plot training history\n",
    "    plt.figure(figsize= (20, 8))\n",
    "    plt.style.use('fivethirtyeight')\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(Epochs, tr_loss, 'r', label= 'Training loss')\n",
    "    plt.plot(Epochs, val_loss, 'g', label= 'Validation loss')\n",
    "    plt.scatter(index_loss + 1, val_lowest, s= 150, c= 'blue', label= loss_label)\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(Epochs, tr_acc, 'r', label= 'Training Accuracy')\n",
    "    plt.plot(Epochs, val_acc, 'g', label= 'Validation Accuracy')\n",
    "    plt.scatter(index_acc + 1 , acc_highest, s= 150, c= 'blue', label= acc_label)\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03cb8895-18db-4ba6-b13b-d49afd8f1788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize= False, title= 'Confusion Matrix', cmap= plt.cm.Blues):\n",
    "\t'''\n",
    "\tThis function plot confusion matrix method from sklearn package.\n",
    "\t'''\n",
    "\n",
    "\tplt.figure(figsize= (10, 10))\n",
    "\tplt.imshow(cm, interpolation= 'nearest', cmap= cmap)\n",
    "\tplt.title(title)\n",
    "\tplt.colorbar()\n",
    "\n",
    "\ttick_marks = np.arange(len(classes))\n",
    "\tplt.xticks(tick_marks, classes, rotation= 45)\n",
    "\tplt.yticks(tick_marks, classes)\n",
    "\n",
    "\tif normalize:\n",
    "\t\tcm = cm.astype('float') / cm.sum(axis= 1)[:, np.newaxis]\n",
    "\t\tprint('Normalized Confusion Matrix')\n",
    "\n",
    "\telse:\n",
    "\t\tprint('Confusion Matrix, Without Normalization')\n",
    "\n",
    "\tprint(cm)\n",
    "\n",
    "\tthresh = cm.max() / 2.\n",
    "\tfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "\t\tplt.text(j, i, cm[i, j], horizontalalignment= 'center', color= 'white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "\tplt.tight_layout()\n",
    "\tplt.ylabel('True Label')\n",
    "\tplt.xlabel('Predicted Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ea4c144-cc17-4b3b-b792-65d7e246209a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1978 validated image filenames belonging to 2 classes.\n",
      "Found 247 validated image filenames belonging to 2 classes.\n",
      "Found 248 validated image filenames belonging to 2 classes.\n",
      "Generators created successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Function to split data into train, validation, and test sets\n",
    "def split_data(data_dir, test_size=0.2, valid_size=0.2):\n",
    "    try:\n",
    "        # Assuming the dataset is in image files\n",
    "        image_paths = []\n",
    "        labels = []\n",
    "        \n",
    "        # Iterate through subdirectories to create the data paths and labels\n",
    "        for label in os.listdir(data_dir):\n",
    "            label_path = os.path.join(data_dir, label)\n",
    "            if os.path.isdir(label_path):\n",
    "                for image_name in os.listdir(label_path):\n",
    "                    image_paths.append(os.path.join(label_path, image_name))\n",
    "                    labels.append(label)\n",
    "\n",
    "        # Convert to DataFrame for easy splitting\n",
    "        data_df = pd.DataFrame({'image': image_paths, 'label': labels})\n",
    "\n",
    "        # Split the data into training, validation, and test sets\n",
    "        train_df, temp_df = train_test_split(data_df, test_size=test_size, stratify=data_df['label'])\n",
    "        valid_df, test_df = train_test_split(temp_df, test_size=valid_size / (valid_size + test_size), stratify=temp_df['label'])\n",
    "\n",
    "        return train_df, valid_df, test_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in splitting data: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "# Function to create generators for training, validation, and testing\n",
    "def create_gens(train_df, valid_df, test_df, batch_size=16, target_size=(224, 224)):\n",
    "    try:\n",
    "        # Define the ImageDataGenerator for augmenting images\n",
    "        train_datagen = ImageDataGenerator(rescale=1./255, \n",
    "                                           rotation_range=30, \n",
    "                                           width_shift_range=0.2, \n",
    "                                           height_shift_range=0.2, \n",
    "                                           shear_range=0.2, \n",
    "                                           zoom_range=0.2, \n",
    "                                           horizontal_flip=True, \n",
    "                                           fill_mode='nearest')\n",
    "\n",
    "        valid_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "        # Flow the data from dataframes to generators\n",
    "        train_gen = train_datagen.flow_from_dataframe(dataframe=train_df, \n",
    "                                                     x_col='image', \n",
    "                                                     y_col='label', \n",
    "                                                     target_size=target_size, \n",
    "                                                     batch_size=batch_size, \n",
    "                                                     class_mode='categorical')\n",
    "\n",
    "        valid_gen = valid_test_datagen.flow_from_dataframe(dataframe=valid_df, \n",
    "                                                          x_col='image', \n",
    "                                                          y_col='label', \n",
    "                                                          target_size=target_size, \n",
    "                                                          batch_size=batch_size, \n",
    "                                                          class_mode='categorical')\n",
    "\n",
    "        test_gen = valid_test_datagen.flow_from_dataframe(dataframe=test_df, \n",
    "                                                          x_col='image', \n",
    "                                                          y_col='label', \n",
    "                                                          target_size=target_size, \n",
    "                                                          batch_size=batch_size, \n",
    "                                                          class_mode='categorical')\n",
    "\n",
    "        return train_gen, valid_gen, test_gen\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in creating generators: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "# Example usage\n",
    "data_dir = \"E:\\leaf-dieaseas\\proper-dataset\\peper-detection\"\n",
    "\n",
    "try:\n",
    "    # Get splitted data\n",
    "    train_df, valid_df, test_df = split_data(data_dir)\n",
    "\n",
    "    if train_df is not None and valid_df is not None and test_df is not None:\n",
    "        # Get Generators\n",
    "        batch_size = 16\n",
    "        train_gen, valid_gen, test_gen = create_gens(train_df, valid_df, test_df, batch_size)\n",
    "\n",
    "        # Ensure that the generators were created successfully\n",
    "        if train_gen and valid_gen and test_gen:\n",
    "            print(\"Generators created successfully.\")\n",
    "        else:\n",
    "            print(\"Error: One or more generators are None.\")\n",
    "    else:\n",
    "        print(\"Error: Data splitting failed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Invalid Input: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c05455ab-521b-4ac1-a7c6-638ba0b1cadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x2000 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images(train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "731167c0-d29a-46f3-881e-ee935e3260a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg19 (Functional)          (None, 512)               20024384  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,025,410\n",
      "Trainable params: 20,025,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_size = (224, 224)\n",
    "channels = 3\n",
    "img_shape = (img_size[0], img_size[1], channels)\n",
    "class_count = len(list(train_gen.class_indices.keys())) # to define number of classes in dense layer\n",
    "\n",
    "# create pre-trained model (you can built on pretrained model such as :  efficientnet, VGG , Resnet )\n",
    "# we will use efficientnetb3 from EfficientNet family.\n",
    "base_model = tf.keras.applications.vgg19.VGG19(include_top= False, weights= \"imagenet\", input_shape= img_shape, pooling= 'max')\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Dense(class_count, activation= 'softmax')\n",
    "])\n",
    "\n",
    "model.compile(Adamax(learning_rate= 0.001), loss= 'categorical_crossentropy', metrics= ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2832ab58-87ee-48a3-91ac-cd3e6aeb297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16   # set batch size for training\n",
    "epochs = 10   # number of all epochs in training\n",
    "patience = 3   #number of epochs to wait to adjust lr if monitored value does not improve\n",
    "stop_patience = 10   # number of epochs to wait before stopping training if monitored value does not improve\n",
    "threshold = 0.9   # if train accuracy is < threshold adjust monitor accuracy, else monitor validation loss\n",
    "factor = 0.5   # factor to reduce lr by\n",
    "ask_epoch = 5   # number of epochs to run before asking if you want to halt training\n",
    "batches = int(np.ceil(len(train_gen.labels) / batch_size))    # number of training batch to run per epoch\n",
    "\n",
    "callbacks = [MyCallback(model= model, patience= patience, stop_patience= stop_patience, threshold= threshold,\n",
    "            factor= factor, batches= batches, epochs= epochs, ask_epoch= ask_epoch )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60061c8-ee68-4efe-9144-0025af1650ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want model asks you to halt the training [y/n] ?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch     Loss   Accuracy  V_loss    V_acc     LR     Next LR  Monitor  % Improv  Duration\n",
      " 1 /10     1.086   57.230   0.66956  59.919   0.00100  0.00100  accuracy     0.00   1547.83 \n",
      " 2 /10     0.643   63.852   0.40042  92.308   0.00100  0.00100  accuracy    11.57   1691.77 \n",
      "                    processing batch 67 of 124  -   accuracy=  89.430   -   loss:  0.34374 "
     ]
    }
   ],
   "source": [
    "history = model.fit(x= train_gen, epochs= epochs, verbose= 0, callbacks= callbacks,\n",
    "                    validation_data= valid_gen, validation_steps= None, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6b3e5a-f1e8-4734-9343-1a19d019b187",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb1429d-5fc6-4ec5-855e-55442fadcb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_length = len(test_df)\n",
    "test_batch_size = test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\n",
    "test_steps = ts_length // test_batch_size\n",
    "\n",
    "train_score = model.evaluate(train_gen, steps= test_steps, verbose= 1)\n",
    "valid_score = model.evaluate(valid_gen, steps= test_steps, verbose= 1)\n",
    "test_score = model.evaluate(test_gen, steps= test_steps, verbose= 1)\n",
    "\n",
    "print(\"Train Loss: \", train_score[0])\n",
    "print(\"Train Accuracy: \", train_score[1])\n",
    "print('-' * 20)\n",
    "print(\"Validation Loss: \", valid_score[0])\n",
    "print(\"Validation Accuracy: \", valid_score[1])\n",
    "print('-' * 20)\n",
    "print(\"Test Loss: \", test_score[0])\n",
    "print(\"Test Accuracy: \", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d6dbad-a4ae-46d9-b20f-689660d2abbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict_generator(test_gen)\n",
    "y_pred = np.argmax(preds, axis=1)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8838b87-b46d-409c-801b-2fa97e86d559",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_dict = test_gen.class_indices\n",
    "classes = list(g_dict.keys())\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(test_gen.classes, y_pred)\n",
    "plot_confusion_matrix(cm= cm, classes= classes, title = 'Confusion Matrix')\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(test_gen.classes, y_pred, target_names= classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0473a3f3-8fce-4eb6-bf90-7ace68dd96cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model.input_names[0][:-6]\n",
    "subject = 'Pepper-dieseas'\n",
    "acc = test_score[1] * 100\n",
    "save_path = ''\n",
    "\n",
    "# Save model\n",
    "save_id = str(f'{model_name}-{subject}-{\"%.2f\" %round(acc, 2)}.h5')\n",
    "model_save_loc = os.path.join(save_path, save_id)\n",
    "model.save(model_save_loc)\n",
    "print(f'model was saved as {model_save_loc}')\n",
    "\n",
    "# Save weights\n",
    "weight_save_id = str(f'{model_name}-{subject}-weights.h5')\n",
    "weights_save_loc = os.path.join(save_path, weight_save_id)\n",
    "model.save_weights(weights_save_loc)\n",
    "print(f'weights were saved as {weights_save_loc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b2e65d-9ed6-4f14-9282-085f586b0703",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = train_gen.class_indices\n",
    "img_size = train_gen.image_shape\n",
    "height = []\n",
    "width = []\n",
    "for _ in range(len(class_dict)):\n",
    "    height.append(img_size[0])\n",
    "    width.append(img_size[1])\n",
    "\n",
    "Index_series = pd.Series(list(class_dict.values()), name= 'class_index')\n",
    "Class_series = pd.Series(list(class_dict.keys()), name= 'class')\n",
    "Height_series = pd.Series(height, name= 'height')\n",
    "Width_series = pd.Series(width, name= 'width')\n",
    "class_df = pd.concat([Index_series, Class_series, Height_series, Width_series], axis= 1)\n",
    "csv_name = f'{subject}-class_dict.csv'\n",
    "csv_save_loc = os.path.join(save_path, csv_name)\n",
    "class_df.to_csv(csv_save_loc, index= False)\n",
    "print(f'class csv file was saved as {csv_save_loc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d129e699-e836-48a8-9610-4403287aa94c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
